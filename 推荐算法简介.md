## 内容简介

1. 信息过载：即内容超过可以遍历的情况
2. 推荐系统：当用户没有明确的目标且信息过载的情况下，将内容以特定的排序返回给用户
3. recall和rank两种方式构建；评估分为线上线下两种
4. 个性化召回：根据用户的操作返回的是推荐全集里面的个性化候选集
   1. 基于用户行为
   2. 基于user profile，即UP
   3. 基于隐语义
5. 整体流程就是match-rank-strategy

---

## LFM，latent factor model，基于邻域

1. 根据用户是否点击item来构建

   |       | Item1 | Item2 |
   | ----- | ----- | ----- |
   | user1 | 1     | 0     |
   | User2 | 0     | 1     |

   根据上面的情况可以看出用户对于item有点击和没点击两种情况，且我们根据item中的影响因素构建好向量，即`user1 = [0.1, 0.2, 0.3]`和`item1 = [0.2, 0.3, 0.4]`假设有三个影响因素，然后矩阵相乘为p，当我们的影响因子产生的结果越接近1的时候，影响因子越正确；

2. loss function，即看1或者0和我们估计出的东西的差值，为了防止过拟合要添加平方项

3. 梯度下降，即loss对变量求偏导

4. 模型影响参数：

   1. 负样本：即用户没有点击的样本，与点击数量一致且从展现给用户的样本里面挑选
   2. 隐特征：f，设置在10-32；正则参数，即平方项的α和梯度下降里面的learning rate β，在0.01-0.05

5. LFM是监督学习，本身离线计算难

6. 其他内容看具体代码

---

## 基于图的个性化召回

1. 用户的行为可以看成是二分图：二分图，即在无向图当中，可以将顶点分成两组，所有路径都是两组之间，即A和B两组，路径都是A和B之间连接；对应推荐里面就是用户和item，操作永远是用户执行item；
2. 构成二分图后，可以通过联通路径看推荐的item，比较时候有一下几个指标
   1. 分别有几条路径联通，A-a-B-b，这么走
   2. 联通路径的长度，上面长度就是3
   3. 联通路径经过顶点的出度，出度就是有几条路径出去，上面出度就是4个顶点的出度和
3. 算法：即认为每个点有α的概率出去且有1-α的概率回来，所以一个点的PR有两部分构成；带来的问题就是一个点要对全局求导，所以引入矩阵；说了这么多，还是没看懂

---

## itme2vec实现召回

1. 和之前的user2item不同，现在是item2item，采用的是NN，网络更深
2. 模仿的是word2vec，将用户的行为变成item构成的语句，也就是embedding
3. 问题：
   1. 根据论文，用户的行为有序和无序产生的结果没差，即丢失了用户行为时效
   2. 用户对于item的操作没有强度区分，例如看视频1%和99%都表示成1
4. 主流程
   1. 利用Log获取用户的行为
   2. 将内容放到word2vec里面产生embedding层数据
   3. 根据embedding的内容，找到对应点的top related，然后返回；
5. word2vec模型：
   1. CBOW，continuous bag of words
      1. 总共有三层，输入层，投影层和全连接层
      2. 比较的是，特定单词w和w的上下文，比如w-2到w+2，将这四个作为输入经过NN和原来w的数值比较，从而产生概率
      3. 采用的是负样本
      4. 本身不是保存modal，而是每次都算
   2. Skip gram
      1. 和CBOW相反，以w作为输入
      2. 自己是通过一个产生4个值，需要对4个值分别负采样，和CBOW正好反过来
   3. loss func采用的是对数，
6. 负采样算法
   1. 根据单词在句子中出现的频率决定当前单词长度，然后将一个句子分成0-1
   2. 将0-1等分成1e-8段，然后进行随机，随机到目标单词的区域则跳过，不在则加入到负样本当中

---

## 基于内容的推荐算法

1. 优点是推荐准确，缺点是需要用户的操作量且领域收束
2. 主体流程，前两步主要是在于NLP和用户画像的范畴
   1. 刻画item profile
      1. Topic，利用title和内容分词，去找关键词，然后给出top描述
      2. Genre，利用分词和textCNN，产生不同分类下的权重，然后加权；视频这种是要找关键帧，然后图像识别进行分类，另外配合音频分类进行加权
   2. 刻画user profile
      1. Genre/ topic：采用的是统计的方式
      2. Time decay，不同时间更改权重
      3. 产生的是用户对于不同种类的倾向性
   3. 线上推荐
      1. 根据用户拿到top k Genre/ Topic
      2. 在构建genre下的item打分，根据这些权重返回genre下的best n，权重越高N越多

---

## 测评

1. 线下：即将实验数据分为8-2这种，8去训练，2来测试，看推荐的和点击的比例，越高越靠谱，可以考虑线上评测
2. 线上：指定观测指标，采用abtest

---

## 排序内容

1. 学习排序，即通过模型和参数模拟单个文章的点击次数，然后进行排序；定义：通过召回得到的候选集，结合用户属性，上下文等特征展现优先级的过程
2. 召回决定效果的天花板，排序就是逼近这个天花板
3. 排序
   1. 预排序：即粗略的对召回内容排序，然后放到系统里进行主要排序
   2. 主要排序
   3. 重排，即将得到的结果和用户最近的行为结合进行再次排序
   4. 从单一浅层到多层到现在的深度

---

## 逻辑回归模型

1. 流程如下
   1. 点击率预估和模型分类：产生的结果使用数值表示，如0，1就是二分类
   2. 逻辑回归拟合，`w = ax1 + bx2 +cx3 + d`这样的形式进行多参数的拟合，之后再将参数代入即可求到结果
   3. `y = sigmoid(w)` 函数特性是中间两边可以急速的变化，从而产生较大的特征差异
2. 训练流程
   1. log中获取训练样本
   2. model参数学习
   3. model预测
3. 优缺点
   1. 易于理解
   2. 欠拟合，即没有交叉特征，需要自己补充
4. Loss function，调整参数使用
5. 过拟合：样本没有正确放映现象；所以对于上述的loss function要添加正则化参数，两种方式如下
   1. 所有权重的和乘上α
   2. 权重的平方乘上α
6. Corpus语料
   1. 选样规则：正负比例，如平均下来用户的三次访问导致购买，这样正负就应该是1：2；对于行为少的用户，给与高权重，使得行为达到最低要求
   2. 样本过滤：取消爬虫或者假内容；预处理内容过多的东西
   3. 例子：分析下来菜单的时候，到最后一个1就可以截断了，因为1之后的内容不确定用户是否观看且会增加负样本
7. 特征
   1. 覆盖率和准确率
   2. 选择：常识，以前模型的基础
   3. 预处理：缺省值处理使用均值；归一化到0-1；离散化，如果是分段的，就表示成`[0,1,0,0]`这样的，体现出属于第二个段位，不一定是均分的。不是分段的，1就表示该例的值
   4. 组合特征：将两个特征维度相乘，即向量的乘积
8. 表现标准：auc，看如下的例子
   1. 比如测试集的结果是1,1,1,0,0。给出的评估概率是0.9,0.8,0.3,0.2,0.4
   2. 正负对是3*2 = 6
   3. 正样本概率高于负样本的个数是2+2+1=5
   4. 结果就是5/6
   5. 一般认为高于70%就比较靠谱；
9. 代码没有写全

---

## 浅层排序模型GBDT，gradient boosting tree

1. 决策树：根据特征构建，重点在于优先特征选择
2. 决策树构造：CART，分类树和回归树合并
3. 分类树看基尼指数，回归树采用平方误差最小化；
4. 没有看完





































