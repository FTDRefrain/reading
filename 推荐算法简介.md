## 内容简介

1. 信息过载：即内容超过可以遍历的情况
2. 推荐系统：当用户没有明确的目标且信息过载的情况下，将内容以特定的排序返回给用户
3. recall和rank两种方式构建；评估分为线上线下两种
4. 个性化召回：根据用户的操作返回的是推荐全集里面的个性化候选集
   1. 基于用户行为
   2. 基于user profile，即UP
   3. 基于隐语义

---

## LFM，latent factor model，基于邻域

1. 根据用户是否点击item来构建

   |       | Item1 | Item2 |
   | ----- | ----- | ----- |
   | user1 | 1     | 0     |
   | User2 | 0     | 1     |

   根据上面的情况可以看出用户对于item有点击和没点击两种情况，且我们根据item中的影响因素构建好向量，即`user1 = [0.1, 0.2, 0.3]`和`item1 = [0.2, 0.3, 0.4]`假设有三个影响因素，然后矩阵相乘为p，当我们的影响因子产生的结果越接近1的时候，影响因子越正确；

2. loss function，即看1或者0和我们估计出的东西的差值，为了防止过拟合要添加平方项

3. 梯度下降，即loss对变量求偏导

4. 模型影响参数：

   1. 负样本：即用户没有点击的样本，与点击数量一致且从展现给用户的样本里面挑选
   2. 隐特征：f，设置在10-32；正则参数，即平方项的α和梯度下降里面的learning rate β，在0.01-0.05

5. LFM是监督学习，本身离线计算难

6. 其他内容看具体代码

---

## 基于图的个性化召回

1. 用户的行为可以看成是二分图：二分图，即在无向图当中，可以将顶点分成两组，所有路径都是两组之间，即A和B两组，路径都是A和B之间连接；对应推荐里面就是用户和item，操作永远是用户执行item；
2. 构成二分图后，可以通过联通路径看推荐的item，比较时候有一下几个指标
   1. 分别有几条路径联通，A-a-B-b，这么走
   2. 联通路径的长度，上面长度就是3
   3. 联通路径经过顶点的出度，出度就是有几条路径出去，上面出度就是4个顶点的出度和
3. 算法：即认为每个点有α的概率出去且有1-α的概率回来，所以一个点的PR有两部分构成；带来的问题就是一个点要对全局求导，所以引入矩阵；说了这么多，还是没看懂

---

## itme2vec实现召回

1. 和之前的user2item不同，现在是item2item，采用的是NN，网络更深
2. 模仿的是word2vec，将用户的行为变成item构成的语句，也就是embedding
3. 问题：
   1. 根据论文，用户的行为有序和无序产生的结果没差，即丢失了用户行为时效
   2. 用户对于item的操作没有强度区分，例如看视频1%和99%都表示成1
4. 主流程
   1. 利用Log获取用户的行为
   2. 将内容放到word2vec里面产生embedding层数据
   3. 根据embedding的内容，找到对应点的top related，然后返回；
5. word2vec模型：
   1. CBOW，continuous bag of words
      1. 总共有三层，输入层，投影层和全连接层
      2. 比较的是，特定单词w和w的上下文，比如w-2到w+2，将这四个作为输入经过NN和原来w的数值比较，从而产生概率
      3. 采用的是负样本
      4. 本身不是保存modal，而是每次都算
   2. Skip gram
      1. 和CBOW相反，以w作为输入
      2. 自己是通过一个产生4个值，需要对4个值分别负采样，和CBOW正好反过来
   3. loss func采用的是对数，
6. 负采样算法
   1. 根据单词在句子中出现的频率决定当前单词长度，然后将一个句子分成0-1
   2. 将0-1等分成1e-8段，然后进行随机，随机到目标单词的区域则跳过，不在则加入到负样本当中

---

## 基于内容的推荐算法

1. 优点是推荐准确，缺点是需要用户的操作量且领域收束
2. 主体流程，前两步主要是在于NLP和用户画像的范畴
   1. 刻画item profile
      1. Topic，利用title和内容分词，去找关键词，然后给出top描述
      2. Genre，利用分词和textCNN，产生不同分类下的权重，然后加权；视频这种是要找关键帧，然后图像识别进行分类，另外配合音频分类进行加权
   2. 刻画user profile
      1. Genre/ topic：采用的是统计的方式
      2. Time decay，不同时间更改权重
      3. 产生的是用户对于不同种类的倾向性
   3. 线上推荐
      1. 根据用户拿到top k Genre/ Topic
      2. 在构建genre下的item打分，根据这些权重返回genre下的best n，权重越高N越多









































