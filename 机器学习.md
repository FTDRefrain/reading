1. 内容简介

   1. 简介：将无序数据产生价值的过程

   2. 应用：

      1. 分类问题：图像识别，垃圾邮件识别；给出的是类别
      2. 回归问题：估价预测，房价预测；给出的数字
      3. 排序问题：点击率预估，推荐
      4. 生成问题：图像生成，风格转换，文字描述生成

   3. 算法：卷积，生成里面多；循环，不定长用的多；深度学习+强化学习

   4. 神经网络

      1. 神经元：多个输入产生一个输出；不同的特征值加权，补上常数项，再通过激活函数产生值

      2. 激活函数：产生非线性模型

         1. Sigmoid，值在0和1之间，对于单神经元能看成是二分概率事件的模型；对于多神经元，则是当前的值除以x加和，例子如下，即三个的概率

            ```javascript
            let Y = [y0, y1] = [2.8, 1.3]
            let eWx = [e28, e13]
            let sum = 2.8*e28 + 1.3*e13 + 1
            let Py0 = 2.8*e28 / sum
            let Py1 = 1.3*e13 / sum
            let Py2 = 1 / sum
            ```

      3. 目标函数，也是损失函数，衡量对数据的拟合程度

         1. y-modal(x)去看概率，具体的例子如下

            ```javascript
            [x, y] = [[1,2,3,4], 3]
            modal(x) = [0.1, 0.2, 0.3, 0.4, 0.5]
            // 如果y是二分就不用转换了
            // onehot下，根据y的label值，转换成矩阵
            y = [0, 0, 0, 1, 0]
            y - modal(x) = [0.1, 0.2, 0.3, 0.6, 0.5] = 1.7
            ```

         2. 上面的过程直接使用softmax函数进行概率分布计算

         3. 目标函数分类

            1. 平方差损失函数
            2. 交叉熵，即yln( modal(x) )求和后平均

         4. 找最小值的过程采用梯度下降`k = k -αQ`Q是L对于k即所有参数集合的偏导，就是之前说的w和b，α是学习率

         5. 学习率

            1. 过大：先下降后上升，因为会越过低点
            2. 过小：太慢
            3. 大了一些：找不到最低点

      4. Tensorflow

         1. CIFAR-10进行图片分类训练：

            1. 图片展开方式，RR-GG-BB，对于32X32，就是1024+1024+1024

            2. 整理代码如下

               ```python
               image_arr = data['data'][100]
               image_arr = image_arr.reshape((3, 32, 32)) # 32 32 3
               image_arr = image_arr.transpose((1, 2, 0))
               matplot.imshow(image_arr) # 展示图片
               ```

            3. 图片的像素0-255要做预处理进行归一化，即x / 127.5 - 1就好

            4. 看测试集上面的准确性的时候是隔一定步数，比如5000次以后，拿出20个测试集进行平均准确度的计算

            5. 流程如下：

               1. 本身是声明式，先搭建计算图然后加数据
               2. [None, 3072]形式矩阵的x，[3072，1]形式的w，[None, 1]形式的b，y，构建loss和optimizer，初始化网络，设定步长和训练次数，搭建训练流程；这里的1对应的是一个类别，多类别就是多维度
               3. 单类别的激活函数是sigmoid；多类别的是softmax，且这时候要将y进行one_hot编码
               4. 读取图片，根据2提取像素矩阵，根据3归一化处理，得到图片的像素矩阵
               5. 进行1时候也拿到对应的label，得到x对应的y
               6. 4中的数据作为输入放到2搭建好的模型当中进行训练

         2. jupyter包进行程序

2. 卷积神经网络

   1. 多层网络
      1. 单层效果不行，使用更多层，就是一层一层加过去，上面说的偏导则采用链式法则可以计算出来，所以每次都计算所有层上面的偏导来减小loss
      2. 问题就是耗时且占内存；提出单样本，但是会过于收敛于这个单样本；提出随机的mini-batch作为样本
      3. Mini-batch产生的问题是梯度下降时候产生的震荡问题；局部极值点，因为有多个极值，在一个极值的周围会来回运动而跳不出去；saddle point，即某一点的偏导是0，这样点就停在那里
      4. 引出了动态梯度下降的方式，就是说将之前的梯度下降动量积累（类似动量求和后平均）和这一步的梯度下降动量相加；这样到达极值点的时候，因为有之前的动量积累就会跳出去；而局部震荡时候，两个震荡向量相加会使得点直接靠近内圈，加速了下降的过程
   2. 卷积网络
      1. 常规网络在识别图片时候的问题
         1. 图片的参数很多，导致生成的网络参数更多，带来的就是表达能力强，所以样本小的时候很容易过拟合
         2. 参数过多在梯度下降的时候容易产生问题，因为单个步骤很容易陷入到局部极值
      2. 解决方法，
         1. 利用就是图片局部有相关性的特地，将一片区域，比如10X10放到一起对一个神经元，而不是全连接，这样就减少了参数
         2. 参数共享，特定区域只是内部有关系而与位置无关，强制所有的10X10参数相同，这样就相当于是使用一个map不断扫过图片区域，和图片矩阵进行点积，得到一个小的图片矩阵作为这个图片的特征
         3. 2中的map就是卷积核，输入图像矩阵里面的值是位置，扫过的时候还有步长，即每次跳几个格子
         4. 一个问题是最后的矩阵会变小，可能缺失信息；padding就是说周围包裹若干层0，这样扫过得到的结果依旧还是原来图片矩阵大小的情况；
         5. 卷积核的意义是提取不同位置的特征
      3. 多通道：即有多通道卷积核，不同层的参数不共享，最后是每个通道上的结果和
      4. 结果多通道，即使用复数的多通道卷积核，产生的结果就是多通道的结果；也就是说多通道结果是不同提取特征手段产生的结果
      5. 激活函数，上面产生的结果经过激活函数，得到最后的值
         1. 很多种，sigmoid, tanh, ReLU, Leaky ReLU, Maxout, ELU
         2. 一般使用ReLU，即小于0是0，大于0就是x，因为快
         3. 单调和非线性：线性的话，多层网络可以合并，这样变深没有意义
      6. 池化，即用定长的map去扫，根据不同方式提取map中的特征值
         1. 一般按照平均值或者最大值
         2. 扫的时候不重叠且没有padding的过程
         3. 减少计算量，本身参数按照卷积核和步长来，不同也行
         4. 减少了平移问题，即移动后特征值一样的结果
         5. 损失了精度，即空间信息
      7. 全连接层，即将卷积层先展开成1维向量，然后和神经元11联通；注意后面不能再连卷积或者进行pooling，因为没有二维信息，本身参数很多
      8. 最后不做全连接层，进行反卷积的话生成和目标相同大的图片，用来进行img2img

3. CNN进阶

   1. 不同的模型使用场景不同，且各有优点和缺点，要相互借鉴
   2. 进化流程
      1. 变深变宽 - AlexNet到VGGNet
      2. 不同的模型结构 - VGG到EesNet
      3. 组合模型 Inception + ResNet
      4. 自学习 NASNet
      5. 手机端特化，MoblieNet
   3. AlexNet
      1. 开始的时候进行分层使用复数的GPU进行计算，经过一个卷积层后将结果合并在分别计算，最后进全连接
      2. 首次使用ReLU；2-GPU并行结构；1，2，5卷积层后跟随max-pooling层；两个全连层使用了dropout
      3. Dropout，即每次训练的时候随机mark一些本次不参与计算的神经元；即加速又避免过拟合
         1. 组合解释：每次相当于是使用了子网络，而最后训练的结果就是子网络的组合结果，组合的效果比较好
         2. 数据解释：因为减少了神经元也能拟合，相当于是一个图片进行了变化后去训练网络，所以数据增强了
         3. 动机：消除了网络中神经元的依赖关系，增强了网络本身的泛化效果
      4. 过拟合：对于样本的你和效果好但是对于新来的样本你和效果不好
      5. 参数相关：
         1. 数据增强，对于一张图片，[255, 255]随机得到大量的[224, 224]的结果
         2. Dropout = 0.5
         3. SGD里面动量积累的系数是0.9
         4. 学习率开始0.01，设定一定步长后变为之前的十分之一
         5. Batch-size是128
         6. 一共7个CN，即组合模型
   4. VGGNet
      1. 优点
         1. 更深：使用大量的3X3
         2. 提出视野域的概念：即两个3X3是5X5，两个5X5是两个7X7，要多用3X3，因为视野一样且多一次利用激活函数做的非线性变换，降低了参数
         3. 1X1可以看成是非线性变化：相当于是降低通道数目且提取特征，用1X1去做是将多通道上的单个位置进行激活函数处理，然后降低通道数
         4. 没经过一个pooling层应该将通道数目翻倍
      2. 数据增强方面：先扩大后再随机选[224, 224]
      3. 训练方面，因为是多层次的，所以可以先训练浅的，然后初始化后再训练深的
   5. ResNet
      1. VGGNet的问题：实验发现加深网络后错误率反而上升了
      2. 提出的想法就是，并不是深层学不到东西而是优化没有做好，所以对于深层，采用F(x) + x，然后再进激活函数的方式，这样深层网络至少能学到之前的东西
      3. 流程如下：普通卷积层，pooling，残差结构，跳过全连接直接输出
      4. 好处：
         1. 减少每层学习内容，之前是下一层要记住上一层的位置信息，所以是map-pooling-map-pooling，现在因为有残差，所以可以直接拿到上一层的信息
         2. 不同层的结构相近，有助于学习；因为跨层进行非线性变化的时候可能产生数值结构不同的问题，如[0, 512]到[512, 1024]这种，但因为有了x，所以F(x)小的时候，两层结构相近，这样不用同时兼顾两层，学习速度加快
      5. 多层时候数据经过两个网络后会降低维度，所以原来的x也要降维，使用pooling，之后因为维度降低但是通道数翻倍了，所以要padding增加上等大小的padding
      6. Global-pool，大小等于图的大小，即经过这个过程后图就是原来所有值的均值了
   6. InceptionNet
      1. 解决的是层次加深过拟合和错误率上升的问题；
      2. 稀疏矩阵类似dropout，减少了参数但是没有减少计算量，因为稀疏矩阵采用密集计算效率更高，所以还是有问题
      3. 提出了分组卷积，即采用不同的卷积核依次进行计算，之后把结果合并；例如本来是一个3X3然后产生400通道，现在就是1X1, 3X3, 5X5, 3X3 pooling四种方式各100个通道；这样减少了参数也减少了计算量
      4. 实际中注意pooling后记得padding
      5. 优化方案：比如5X5可以变成3X3；100通道可以通过多加一个1X1来减小通道；3X3也可以优化成1X3和3X1的矩阵；再加上残差
   7. MoblieNet
      1. 分组卷积的时候，每个神经元只做部分的通道而不是全部的通道，得到结构后再合并在一起
      2. 精度下降了但是速度变快参数减少

4. CNN参数问题

   1. 优化算法
      1. 梯度下降的问题：受限于初始学习率，且全局学习率一样的问题；
      2. AdaGrad算法：学习率除上以前所有梯度和的开方；这样开始很快后面比较慢，不同参数的学习率也不同；问题就是初始学习率有影响，因为会跳的过大，且后期会提早结束学习，因为梯度下降值大
      3. RMSProp，采用的是均值梯度
      4. Adam，将上面两个合在一起
      5. 自适应，即自己设置学习率的改变方式
      6. 对于稀疏数据，自适应的比较好
      7. 包括activation, initializer, optimizer
         1. Activation: relu, sigmoid, tanh等
         2. Initializer：he, Xavier, normal，truncated_normal
         3. Optimizer: adam, gradient descent, momentum；产生的结果不好的原因，可能是初始化参数不好，未训练完全，学习率的问题
   2. 激活函数
      1. sigmoid问题：均值不是0；左右两侧快速收敛；计算复杂；反向传播的时候，因为本身导数是小数相乘，所以链式求导下会导致梯度消失
      2. Tanh：没有梯度，计算复杂，但是均值是0
      3. ReLu：计算简单，均值不是0，梯度为1不会饱和，收敛速度快，问题是小于0的部分会无法激活一直是0
      4. Leaky-ReLU：解决了小于0无法激活的问题
      5. ELU：类似上面，不过均值更接近0，但是计算量大
   3. 网络初始化
      1. 好的初始化方法是，经过多层NN，每次上面的分布都大概是正太分布到-1和1的区间内
      2. 不同的初始化方法对于不同的激活函数的效果不同
   4. BN，批归一化：
      1. 即对于每一个batch里面的数据进行归一化处理后再进行激活。
      2. 首先得到当前batch的x均值，然后将所有的x都减去均值，完成了批归一化；
      3. 问题就是因为一个batch不能反映样本的情况，这样已经算好的网络就失效了，所以提出了逆归一化的思想，即将两个参数，对于上述产生的x并不是直接返回，而是引入两个参数，均值和方差，两个参数通过深度学习获得；注意这个参数在测试时候要关闭训练，即使用训练好的值
      4. Conv -> bn -> activation
   5. 数据增强
      1. 归一化的方式
      2. 图像变化：翻转flip，拉伸resize，裁剪crop，变形
      3. 色彩变化：对比度和亮度brightness & contrast
      4. 多尺度裁剪，即上面的255变成224；流程就是随便选择数字，将图片变大然后去裁剪，获得最后的224
      5. 图片可以进行复数的增强方式，即翻转，拉伸，亮度改变都作用上
      6. tensorflow数据是四维的，[样本数， 长，宽，通道数]
   6. 尝试正则化W，比如平方或者绝对值
   7. tensorBoard进行中间状态的输出
   8. Fine-tune，即利用已经训练好的模型作为初始化内容；每一步存储models，开始训练的时候重新装载，保持一些层级不变只训练一部分层级

5. 图像风格转换

   1. V1算法
      1. 各层学习内容，每层都是图像的抽象表示，众多的抽象组合起来产生了目标
         1. 第一层，色彩相关
         2. 边缘信息，如边线，圆圈等
         3. 四层，目标形状等；三层是pooling
         4. 背景相关内容
      2. 内容特征+风格特征两组输入，第一个是某一层的激活值，第二个是某一层激活值之间的关系；层次低的时候内容特征好，层次高风格特征好
      3. 图像生成流程：固定CNN的参数，将原图片里的像素值随机打散，通过网络生成y_，对于原来图片经过网络产生的y，两者有个loss，不断的调整开始随机产生的像素使得这个loss变小；
      4. 内容特征损失函数：上述流程，计算方式是平方差均值
      5. 风格特征：Gram矩阵，将不同卷积核提取到的信息矩阵两两进行内积得到最后的值，然后比较随机生成的和原图的平方差均值
      6. 将两个损失函数加权求值，然后不断的进行梯度下降的过程就能产生最后的结果
      7. 问题是计算慢；优点是中间状态多可以自己挑选；但是太慢就是缺点
      8. 实战流程
         1. 用已经训练好的网络模型参数搭建好网络模型
         2. 构建Build函数，对于输入存储每层的输出值
         3. 随机产生一个图片结果值，均值是127.5，方差是20；使用build读取内容图片，风格图片和结果图片，产生三个实例
         4. 内容损失函数，利用前两层，计算每一层的平方差均值
         5. 风格损失函数，构建q = [h*w, channel]，两两相乘的结果就是q的转置和q做乘法，得到了[ch, ch]
         6. 两者的权重选择的时候尽量使得两者在同一个数量级上面
         7. 随机图片->网络得到各层数值->计算内容和风格的loss->loss放进去optimizer->通过网络得到输入值最后的结果
   2. V2算法
      1. 区别在于结果图像不是初始化的，而是由内容图像经过一个网络得到的，后半部分一样，最后变成训练这个产生结果图像的网络；使用的时候就是风格图像不变，不同的内容图像产生想要的结果
      2. 优点
         1. 是对于已经搭建好的网络，之后就是直接利用开始的网络产生结果就好了而不用再次随机图片然后计算
         2. 扩展性，当风格转化不设置的时候，就可以实现低清晰到高清晰的转化，即训练的时候输入是低清晰图像，通过网络得到结果，然后结果和高清loss最小
      3. Information trans net
         1. 里面没有使用pooling而是使用step为2的卷积核替代，因为pooling的功能可以通过卷积核替代，然后fractional strided即step为小数的卷积，可以实现逆操作
         2. Downsampling and upsampling，即上面两种不同strided方式产生的结果
         3. 使用了5个residual block
         4. 输出层使用了tanh，保证输出值在[0, 255]
         5. 一个9X9，其他都是3X3
         6. Down-up的优点：
            1. 减少了feature-map的大小
            2. up的过程扩大了视野域，因为down的过程利用两个3X3等于5X5的过程，在up的过程中就提高了视野
            3. 残差能学到一部分的共享信息
         7. 缺点就是风格loss里面的grad矩阵不好解释，没法改进
   3. V3算法
      1. 将之前产生的结果分成若干个patch，内容的loss是序列相同patch块进行平方差，风格loss是找到最匹配的patch进行方差
      2. 这样更改图片的时候相当于不断的更改若干个patch

6. 循环神经网络

   1. 解决是不定长的问题，即一对多，多对一，多对多等

   2. 最基本的公式如下，即上一次的结果和本次的输入，通过加权然后激活函数得到本次的结果，最后经过softmax；这样的话就是每个中间的状态都有对应的值，且多输入的时候就依次把多输入放进去；

      ```javascript
      st = fw * (st-1, xt)
      st = tanh(W * st-1, U * xt)
      y_ = softmax(V*st)
      ```

   3. 字符语言模型：即输入后帮助预测下一个字符

      1. 流程如下：首先分词，拿到每一步在分词矩阵上的值，然后输入到网络，得到当前步的状态在分词矩阵上的值，利用softmax算出概率得到结果。下一步的输入就是下一步的原始值和上一步的状态值一起放进网络。重复上面的过程
      2. 测试过程：将上一步的输出作为本次的输入放进去
      3. 可能的问题：之前结果错，后面都错，但因为有加权的问题，所以之前结果的影响并没有特别的大，有兼容性存在

   4. 正向传播的时候是计算当前层的值，每层的梯度是之前所有的梯度加和，然后loss就是所有的值加和

   5. 反向传播：链式求导

   6. tanh的问题是输出在-1到1之间，梯度会下降，就是说远距离的贡献小。计算的时候可以考虑直接忽略。另外不适用tanh的话，由于中间态的原因，大量的计算会产生问题

   7. 扩展

      1. 之前每一个中间态只有一层，自然想到扩展成多层且使用残差
      2. 双向：即一层正向的，一层反向的，然后两层中间态结果加和作为最后的结果；实时用不了，因为还没有之后的结果

   8. 问题：对于一层的网络参数W，因为后面都要使用，在传播中承载的信息过多，这样并不能长久传播

   9. LSTM

      1. 相当于是RNN结构的扩展，引入了选择性机制，即选择性输出，选择性输入和选择性遗忘
      2. 门函数，sigmoid,[0, 1]之间，将输入乘上这个变量
      3. 大概的结构如下
         1. 依旧是不断传递隐藏状态，本身由遗忘门和输出门两个动作
         2. 输入增加了一个下次的结果
         3. 像是输入和上次结果输入遗忘函数，然后和隐藏状态点积，即隐藏状态进入遗忘门：意义大概是有了新的主语就忘记之前的主语
         4. 当前输入进行遗忘门，先把总的输入tanh然后点积实现传入门：即选择遗忘一些输入的内容
         5. 4，5的结果加和得到这次的隐藏状态
         6. 当前的隐藏状态经过tanh再次和遗忘内容进行点积，得到了当前步骤的输出

   10. 文本分类

       1. 大概结构：分词，进入RNN，拿到结果矩阵，放到MLP里面(MLP基本就是一两层的全连接)，得到最后的结果矩阵，然后进行softmax往文本类别上放；
       2. 上面的结构有些问题，会有信息瓶颈，因为最后一个输出和最后的输入及上一个结果关系太大而和初始输入关系太小
       3. 双向LSTM，变成双向；组合输出，即拿到所有的中间状态，通过拼接或者pooling再放到MLP中
       4. 例子：慕课深度学习课程优秀，分词走流程后，判断是积极，是一般还是差评这种
       5. Embedding：将词语变成设定好维度的多维向量，本身是可以训练的
       6. HNN，即先从词语计算句子的向量值，然后句子放进去再计算最后的结果；另外也引入了attention，即对于每个中间状态的输出，利用attention进行遗忘，从词语到句子的中间态都做；
       7. attention做法大概如下：将结果向量和一个矩阵做乘法后成果sigmoid函数变成-1到1之间的值，将这个矩阵值和结果做点积得到了当前结果的attention结果
       8. CNN文本分类
          1. 适用于长度相对于固定的情况下，即通过padding将不同长度的语句补成相同长度的，补内容的时候使用unk；
          2. 对于每一个单词，构建好的多维度向量相当于是在多维度上对于该单词的描述，即对于一个句子，一列是一种描述方式
          3. 多通道一维卷积：即拿卷积核的每列和对应的列进行计算作为该点的值
          4. 利用复数的卷积核进行多通道一维卷积后，就形成了复数通道的feature map
          5. pooling层就是对于每个通道，取最大值或者均值
          6. 经过全连接层后进行softmax得到结果；
       9. embedding可以提前训练好，比如使用word2vec，直接从里面拿出字库；
       10. R-CNN
           1. 对于一个句子来说，根据一个词的位置分为前部分，后部分和词本身，共三种中间状态，词本身参与前部分和后部分的RNN结构。这个就是双向RNN
           2. 对于这个句子，通过CNN得到一个抽象内容，然后max-pooling进行拼接，再到MLP里面得到最后的输出
           3. embedding的问题，分词过多导致的矩阵太大，所以空间占用大，引出了压缩的问题
           4. 压缩方法：共享压缩，即采用两个变量表示分词，这样本来的hash map变成了矩阵表示，参数开方
       11. 实战
           1. 利用训练数据，构建分词字典，去掉里面出现次数少的；构建label字典
           2. 先想模型再写代码
              1. 构建计算图：embedding, LSTM, fc, train_op
              2. 训练流程代码封装
              3. 数据集封装：数据预处理，next_batch
              4. 词表封装：句子转换成id，每个句子对应的是一个列表，即里面单词对应的idx
              5. 类别封装：即输出label字典
           3. 这里注意因为是在mini-batch上面进行训练的，所以还是要控制句子长度一致
           4. 读数据，构建好数据集，词表，类别
           5. 计算图构建：embedding构建，初始化参数；LSTM构建，初始化参数后生成两层LSTM，拿到最后一步的输出；全连接层，设置初始值，dropout率；结果计算softmax之后计算loss；对于loss函数上的不同变量进行梯度下降
           6. 训练流程和之前一样

7. 图像生成文本

   1. 传统采用的是检索方法，问题是对新内容没有结果
   2. 评测指标
      1. BLEU score：采用的是N-gram match方法，即不同词数，从一个词两个词一直到N个词，看match的程度，即出现的词和预测词长度的比值，将结果进行加权和；问题一个是词语都相同导致1维度时候的虚高，所以考虑该词在结果里面出现的词数；另一个问题是预测词语短时候，值虚高，所以引入短句惩罚机制
      2. 人工评测
   3. 架构，Encoder - decoder的方式，即将输入经过网络得到编码，然后再经过产生文本的网络得到最后的结果
   4. decode过程算法：贪心直接使用最大概率不行，因为文字间的关联性不行；
   5. 采用的是beam-search的方法：即每次拿到的是top-k，作为本次的输出，下次拿到了k2的结果后再选top-k。这样最后得到了k个路径，使用单独的语言评估模型去检测这k个路径的好坏就行
   6. 不同的模型结构
      1. Mulit-modal：将图像特征和一个单词作为本次的输入，得到下一次的单词，然后再集合图像特征作为输入
      2. show and tell：全连接层图像特征直接进LSTM
      3. show attend and tell
         1. 引入了attention机制，因为图片上信息和位置有关系，所以从全连接层变成CNN，利用feature map矩阵值，通过加权求和后再输入到LSTM里面，就形成了attention机制
         2. Attention：将向量和上一步的LSTM结果作为输入，经过函数后，去计算该点的值，计算map上面所有点的值后加和，这个比值就是该点的权重。利用权重和向量计算出了这一步的attention值，和向量还有上一个LSTM结果作为输入放入到这一层的LSTM中
         3. 得到上面的attention后就可以在图像中直接找到单词对应的区域
         4. 问题在于，attention本身的网络需要学习，而里面还有上次的LSTM结果，所以网络过载
      4. Top-down Bottom-up attention模型，思想是代码问题能通过分层解决
         1. 做了两层LSTM，即一层用来训练attention，一层用来训练输入
         2. 第一层放入的是图像位置信息的均值，是个固定值；第二层放入的就是attention后的值
   7. 实战
      1. 还是分词，词表构建，label构建，
      2. check point导入进行图片矩阵信息提取
      3. 训练数据方式如下：[img, a, b, c, d] -> [a, b, c, d, e]，前面是输入后面是输出，训练这个网络
      4. 具体代码没有看

8. 对抗神经网络

   1. 换脸，图像修复，文本生成图像，图像翻译，无配对图像翻译，多领域图像翻译等应用
   2. 解决的是无中生有的问题，引入的是判别模型
   3. 生成器和判别器，生成器是要生成模型使得判别器无法判断，而判别器则是不断提高判断生成的内容。最后的结果是生成器完全产生目标且判别器几乎没有效果
   4. 例子如下：正常的结果是正太分布，开始生成器的结果是均值过大。判断器进行训练，得到判断曲线。然后生成器改变，使得上面判断的结果变的不准确。重复这个过程，直达判断器几乎无法判断，这时候生成器产生结果就十分贴近模型
   5. DCGAN，深度卷积对抗网络
      1. 解决的是生成一张真实图像的问题；利用随机的向量和已知的图像配对进行训练，作为输入放进去。训练好的网络对于任意向量来说，生成的图像一定是一张真实的图像；
      2. 流程：明确定义目标->定义G的输入输出->定义D的输入和输出->定义G和D的结构
      3. 生成器G：将向量经过复数的反卷积层，最后得到三通道的图像
      4. 看另一份文件