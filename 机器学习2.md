1. 对抗神经网络

   1. DCGAN

      1. fractional-strided，反卷积：可以理解成先padding，然后使用卷积核扫过；也可以把卷积核看成一维向量，利用矩阵的原理可以知道，反卷积的反向传播梯度下降函数和卷积操作的正向传播梯度下降的过程一样
      2. 反卷积的方式：直接在周围加上padding。另一个是在每一个周围九宫格加上padding，即两个间有间隔。
      3. 将所有的pooling使用卷积替代。D是strided，G是反卷积，因为是向量生成图片，所以是反卷积
      4. 模型结构：
         1. G和D都是用了batch normalization，解决的是初始值不好，梯度下降的问题。经验上讲，不应该作用于输入层和输出层
         2. G上输出层使用的是tanh，其余层使用的是relu
         3. D上使用的是LeakReLu
      5. 应用上，因为是向量，所以存在加减的应用。比如复数有眼镜照片的向量均值减去一个人加上一个人，最后一个人就会戴上眼镜

   2. 图像翻译，Pix2Pix

      1. 即对于同一张图有不同的表达方式，比如背景色改变，实物图变成素描图

      2. G上面变成图像到图像；D上变成两个判断，是否是真实图像，是否是一对图像。即输入的时候是真实的图像对和G生成图像和目标图像对

      3. G结构：U-Net，大概的结构如下，依旧是先编码后解码，两部分对应层上的数据做了一个残差链接，ED是flatten的结果了，即256X256X3到ED的时候就是1X1X512

         ````javascript
         E1 ----------> D1
         	E2 -------> D2
         		E3 ---> D3
         			  ED
         ````

   3. 非配对图像翻译，cycleGAN

      1. 源于语言翻译，即好的翻译就是能翻译过去再翻译回来的。提出两套GAN
      2. 一套用于生成和判断，另一套用于转换回来内容的翻译和判断。
      3. 有效原因：因为转换过去再转换回来相当于加上了一致性约束，减少了搜索空间。
      4. 损失函数，两边GAN的loss加上λ乘上一致性损失函数(和G，F及x, y有关)
      5. 网络细节
         1. D的输入变为70*70来加快速度
         2. GAN的loss里面将里面的参数设置成平方，经验
         3. λ为10
         4. adam优化
         5. α前100次是0.0002，之后线性下降

   4. 多领域图像翻译

      1. 对于多要求的图像翻译，原来的网络仅支持1种到1种
      2. 提出starGAN，即提出一个中心，若干种的变化都可以通过这个中心转化后得到目标
      3. 大部分和之前的pix2pix一样，中间多了一个部分：将input和target domain作为输入放进去得到了fake，将这个fake和input的风格合并通过G得到第二个fake，第二个fake和input要进行一致性损失函数。而第一个fake则进入D去判断真假以及是否属于目标domain
      4. 上面的input和target domain合并的时候需要将domain embedding，考虑本身domain小而图像大，可以考虑加在中间，放头部也没有问题；
      5. 一个区别就是G和D的训练目标函数不同，不再是同一个函数最小化G最大化D了。

   5. 文本生成图像

      1. G的输入是文本+随机向量，因为输入不同可能对的是同一张图片；D的输入是fake加上文本向量，这里注意文本向量要在图像卷积展开中间拼接上去
      2. 训练集构建时候，可以先训练能否判断fake再训练是否匹配描述。另外训练集里面应该有，<真图，描述>，<假图，描述>，<真图，错误描述>

   6. DCGAN实战

      1. 模块构建
         1. Data provider: Image data and random vector
         2. compute graph: generator, discriminator, DCGAN
         3. training process
      2. 具体代码没有看

2. 自动机器学习网络

   1. 自动搜索网络结构；指定任务上通过fine-tune或者迁移学习拿到更好的结果
   
   2. 网络搜索进行条件：搜索空间，搜索反馈，
   
   3.  搜索空间参数例子：
   
      1. 卷积层，filter height/ weight/ number, strided height. Weight，每层都有这5个参数；number表示的是通道数目
      2. 相比于设置区间，直接设置值，如[1, 3, 5, 7]来设置卷积核大小
   
   4. 搜索反馈：使用得到的准确率作为评估值，所以每个网络要进行一步计算得到准确率。即有两个网络，一个网络是将上面五个参数经过RNN进行训练，另一个网络则是使用上面的结果构建训练网络得到输出
   
   5. 得到结果的例子
   
      ```javascript
      net result = [1(0.9), 3(0.3), 5(0.7), 5(0.3), 7(0.8)]
      R = 0.3
      loss of step5 = 0.8 * 0.3
      ```
   
   6. 为了提高训练的效率，采用了分布式训练
   
   7. paramer server结构：
   
      1. Server：接收worker传递过来的梯度，进行所有参数的更新
      2. Worker：分布式结构，从server更新参数，输入网络进行梯度下降计算，得到新的梯度上传到server中
      3. 同步和异步两种更新梯度的方式，采用异步多一些
      4. 这样一个架构上面有20个PS，中间有100个控制器进行PS的冗余来实现并行计算，每个控制器batch为8.
      5. 训练的次数为50固定
   
   8. 版本缺点：
   
      1. 每层的参数独立，即上面五个参数
      2. 复杂网络不适用，五个参数结构单一
      3. N多了之后，搜索空间变大
   
   9. 网络版本二：
   
      1. 使用相同的子结构。每一层是利用不同的操作，比如经过3X3卷积，pooling，或者加和及拼接操作来生成下一层的子结构。
      2. 这样搜索空间就是cell type，operator and connnect method三个部分
      3. 共两个版本的子结构，即reduction and normal，normal是最后一步链接上所有的变换，而reduction则是中间有子项的合并。
      4. 最后的网络就是N个normal，一个reduction。
      5. 将上面的网络重复B次，一般选择B等于5，然后放到实际里面去训练特定步骤
      6. 注意reduction里面每次通道数目翻倍
   
   10. 版本三：
   
       1. 降低搜索空间，减少一些使用次数少的操作
       2. 替代重复B次，在1到B之间，每一个阶段都放进去训练
       3. 启发式搜索：
          1. 将已知网络结构和准确率放进去，得到其他可能网络结构的准确率
          2. 值不一定准，但是排序一定要对
          3. 得到第一步的所有可能的候选值放进去训练启发式网络。拿到下一步的生成算法候选结构，放到启发式网络里面排序准确率，拿出top-k训练网络。将上面top-k的结果放进启发式网络训练启发式网络。重复上面的过程